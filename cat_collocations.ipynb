{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cat_collocations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvQFl0Lb49hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznC-an5e5n7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Nk2Yc504Vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pycodestyle flake8 pycodestyle_magic\n",
        "%load_ext pycodestyle_magic\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KK4Zdbbz-nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install natasha\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2QqLgJR7zKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install joblib\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StMZX2azxXHl",
        "colab_type": "text"
      },
      "source": [
        "Задача: научиться находить в тексте лексически/стилистически неправильные сочетания (коллокации) и предлагать более правильные замены. <br>\n",
        "Алгоритм является улучшенной версией этого алгоритма https://github.com/annadmitrieva/NLP-stuff/blob/master/collocation_generation-Copy4.ipynb "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC5oMw4Dw6NZ",
        "colab_type": "text"
      },
      "source": [
        "#Этапы работы\n",
        "\n",
        "*   Автоматическое обнаружение \"неправильных\" коллокаций в тексте.\n",
        "*   Поиск замен."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DEhQpYgQcXj",
        "colab_type": "text"
      },
      "source": [
        "Скачиваем word2vec модель из rusvectores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwd70RgHSlbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores4/ruwikiruscorpora/ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgD2XXsOQhSy",
        "colab_type": "text"
      },
      "source": [
        "Для определения домена текста используем предобученную на шестиграммах из каждого домена (по 100000 на домен) SVM. F1-score модели – 0.94. </br>\n",
        "Код модели: https://github.com/annadmitrieva/NLP-stuff/blob/master/domain_model.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av2Q34pcTLa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/lyfk0ur0pqvcgqv/domain_model.pkl?dl=0 -O domain_model.pkl\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxSRwGTUWp6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/y35dm5chuv54psg/suggestions.zip?dl=0 -O suggestions.zip\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aisa0dw-W1Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip suggestions.zip\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KeVWEZsTXLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "import joblib\n",
        "from gensim.models import Word2Vec\n",
        "from natasha import NamesExtractor, AddressExtractor, DatesExtractor\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "    'ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz', binary=False)\n",
        "domain_model = joblib.load('domain_model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig3R2wvUpuMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger_ru')\n",
        "nltk.download('punkt')\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phW6KHcuXcQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def domain(string):\n",
        "    domain = domain_model.predict(list(string))[0]\n",
        "    df = pd.read_csv(f'suggestions_{domain}.csv')\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET70OxkDcjI5",
        "colab_type": "text"
      },
      "source": [
        "#Поиск неправильных сочетаний\n",
        "В предыдущей версии этого алгоритма (https://github.com/annadmitrieva/NLP-stuff/blob/master/collocation_generation-Copy4.ipynb) в список плохих коллокаций попадали все встретившиеся в тексте биграммы из существительных или глаголов, которых не было в эталонном списке. В результате отлавливалось много пар слов, вообще не являющихся словосочетаниями (пример – \"работы разрешение\"). Решение: добавить синтаксический парсер. <br>\n",
        "Будем считать плохими коллокациями такие пары \"вершина-зависимое\", которые состоят из существительных и глаголов и которых нет в эталонных списках правильных словосочетаний (для каждого домена текста – свой список).<br>\n",
        "Эталонные списки коллокаций: https://drive.google.com/drive/folders/1k_N-DZ-nLL5ro66-LxIaE4-dRwirdwZh <br>\n",
        "Код, которых использовался при сборе эталонных списков: https://github.com/MariaFjodorowa/catandthekittens/tree/develop/collocations/collocation_frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6x_yisGUKzt",
        "colab_type": "text"
      },
      "source": [
        "Несколько плохих текстов, на которых будем проверять, насколько хорошо находятся плохие коллокации. Источник – примеры плохой лексической сочетаемости из домашнего задания по академическому письму 1 курса ОП \"Фундаментальная и прикладная лингвистика\" НИУ ВШЭ 2017 года."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wTiBajJlZ3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad_text_pol = \"\"\"В то же время, Путин умолчал от египетского населения данные о том, \n",
        "насколько милитаризируется сама Россия и насколько выросли ее военные расходы. \n",
        "Обама улетел из Вашингтона на вертолете, а Байден - на поезде.\"\"\"\n",
        "bad_text_hist = \"\"\"Маккензи ограничивается рамками исследования 1928 – 1943, исследуя преимущественно идеологию Коминтерна сталинской эпохи, \n",
        "перечисляет причины на то, чтобы рассматривать 1928 год как переломный. \n",
        "Александр Шубин посвящает свою книгу большей частью на исследование общественно-политических движений в послесталинский СССР, \n",
        "1953 – 85 годов. В строках читаются напряжение и недоверие власти.\"\"\"\n",
        "bad_text_ling = \"\"\"Следует избегать наводящие вопросы. Тем не менее существовало несколько точек зрения о создании алфавита. \n",
        "Предлагая ту или иную гипотезу, нельзя основываться и ссылаться только на результаты своего собственного исследования. \n",
        "Необходимо внимательное изучение опыта других ученых. \n",
        "Термин морфология обычно соотносится к части грамматики языка. Однако этот термин в истории науки употреблялся и в совсем ином значении.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4hhc4q_VnoM",
        "colab_type": "text"
      },
      "source": [
        "##Подход 1: MaltParser (медленный и плохо работающий)\n",
        "Для POS-тэггинга использовался метод pos_tag из NLTK для русского языка.<br>\n",
        "Для синтаксического парсинга использовался MaltParser из NLTK с предобученной моделью для русского языка. <br>\n",
        "Источник модели: http://corpus.leeds.ac.uk/mocky/ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxqmEIAea2CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://corpus.leeds.ac.uk/tools/russian.mco\n",
        "!wget http://maltparser.org/dist/maltparser-1.8.1.zip\n",
        "!unzip maltparser-1.8.1.zip\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsBBabRMhKnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export MALT_PARSER=$HOME/maltparser-1.8.1/\n",
        "!export MALT_MODEL=$HOME/russian.mco"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6oPPziptvmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pos_tag_rus(tokens):\n",
        "    return nltk.pos_tag(tokens, lang='rus')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGk1vb92ipvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "syntax_parser = nltk.parse.malt.MaltParser(\n",
        "    'maltparser-1.8.1', 'russian.mco', tagger=pos_tag_rus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm1TcbjMrnJM",
        "colab_type": "code",
        "outputId": "3f6622a2-0588-464c-9537-73b5d2f28ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# проверим, правильно ли работает парсер\n",
        "sents = nltk.sent_tokenize(bad_text_pol)\n",
        "nodes = syntax_parser.parse_one(sents[0].split()).nodes\n",
        "for node in nodes.keys():\n",
        "    head = nodes[node]['head']\n",
        "    if head is not None and nodes[head]['word'] is not None:\n",
        "        print(f\"Head: {nodes[head]['word']} {nodes[head]['ctag']}. \" +\n",
        "              f\"Dependent: {nodes[node]['word']} {nodes[node]['ctag']}.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Head: то A-PRO=n. Dependent: В PR.\n",
            "Head: то A-PRO=n. Dependent: же PART.\n",
            "Head: то A-PRO=n. Dependent: время, S.\n",
            "Head: время, S. Dependent: Путин S.\n",
            "Head: от PR. Dependent: умолчал V.\n",
            "Head: Путин S. Dependent: от PR.\n",
            "Head: населения S. Dependent: египетского A=n.\n",
            "Head: то A-PRO=n. Dependent: населения S.\n",
            "Head: населения S. Dependent: данные S.\n",
            "Head: том, S. Dependent: насколько ADV.\n",
            "Head: сама A-PRO=f. Dependent: милитаризируется V.\n",
            "Head: насколько ADV. Dependent: и CONJ.\n",
            "Head: выросли V. Dependent: насколько ADV.\n",
            "Head: выросли V. Dependent: ее S.\n",
            "Head: ее S. Dependent: военные A=pl.\n",
            "Head: военные A=pl. Dependent: расходы. S.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OCtnuWCoFQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search(text, collocations):\n",
        "    \"\"\"text – строка, collocations – датафрейм pandas.\n",
        "    Возвращается словарь candidates,\n",
        "    где ключи – пары слов, а значения – пары тэгов\"\"\"\n",
        "    candidates = {}\n",
        "    first = list(collocations.first_word)\n",
        "    second = list(collocations.second_word)\n",
        "    clean_text = re.sub(\"\\([a-zA-z\\d .,:]*\\)|[a-zA-z\\d]*\", \"\", text)\n",
        "    # чистим текст от библиографических ссылок\n",
        "    # и токенов, состоящих только из латиницы и цифр\n",
        "    extractors = [NamesExtractor(), AddressExtractor(), DatesExtractor()]\n",
        "    named_entities = []\n",
        "    # составляем список всех именованных сущностей, встречающихся в тексте\n",
        "    for extractor in extractors:\n",
        "        matches = extractor(text)\n",
        "        for match in matches:\n",
        "            start, stop = match.span\n",
        "            for i in re.findall('\\w+', text[start: stop]):\n",
        "                named_entities.append(i.lower())\n",
        "    # разбиваем текст на предложения, а предложения на слова\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    for i in range(len(sents)):\n",
        "        nodes = syntax_parser.parse_one(sents[i].split()).nodes\n",
        "        for node in nodes.keys():\n",
        "            head = nodes[node]['head']\n",
        "            if head is not None and nodes[head]['word'] is not None \\\n",
        "                    and nodes[head]['word'].lower() not in named_entities \\\n",
        "                    and nodes[node]['word'].lower() not in named_entities \\\n",
        "                    and nodes[head]['ctag'] in ['S', 'V'] \\\n",
        "                    and nodes[node]['ctag'] in ['S', 'V']:\n",
        "                word_1 = nodes[head]['word'].lower()\n",
        "                word_2 = nodes[node]['word'].lower()\n",
        "                tag_1 = nodes[head]['ctag']\n",
        "                tag_2 = nodes[node]['ctag']\n",
        "                if word_1 in first or word_2 in second:\n",
        "                    if word_1 in first and word_2 in second:\n",
        "                        indices_1 = [i for i, x in enumerate(first)\n",
        "                                     if x == word_1]\n",
        "                        indices_2 = [i for i, x in enumerate(second)\n",
        "                                     if x == word_2]\n",
        "                        if not set(indices_1).isdisjoint(indices_2):\n",
        "                            pass\n",
        "                        else:\n",
        "                            candidates[(word_1, word_2)] = (tag_1, tag_2)\n",
        "                    else:\n",
        "                        candidates[(word_1, word_2)] = (tag_1, tag_2)\n",
        "    return candidates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSOOGGUAGLJW",
        "colab_type": "text"
      },
      "source": [
        "Проверим, как функция search работает на наших плохих текстах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkYt7PDLirB7",
        "colab_type": "code",
        "outputId": "1487f4ca-9b6e-48a1-aa4f-c0d6bb51be05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "candidates_pol = search(bad_text_pol, pd.read_csv('suggestions_pol.csv'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 467 ms, sys: 62 ms, total: 529 ms\n",
            "Wall time: 10.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTHI2zhleizt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b05942f-f077-4a5e-d760-b5b1fd225951"
      },
      "source": [
        "candidates_pol"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('населения', 'данные'): ('S', 'S')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hszw_bq9IkNI",
        "colab_type": "code",
        "outputId": "45a09124-00e8-4dfa-9867-e7acda0076b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "candidates_hist = search(bad_text_hist, pd.read_csv('suggestions_hist.csv'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 252 ms, sys: 66 ms, total: 318 ms\n",
            "Wall time: 16.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAkiH5vie1vW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4e1edcaf-874a-41e3-d059-3bb6561e5dfe"
      },
      "source": [
        "candidates_hist"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('недоверие', 'власти.'): ('S', 'S'),\n",
              " ('ограничивается', 'рамками'): ('V', 'S'),\n",
              " ('перечисляет', 'причины'): ('V', 'S'),\n",
              " ('рамками', 'исследования'): ('S', 'S')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrMh9eD3Q3Sd",
        "colab_type": "code",
        "outputId": "b7dc81e3-161f-4c53-a033-7523be2ab1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "candidates_ling = search(bad_text_ling, pd.read_csv('suggestions_ling.csv'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 448 ms, sys: 142 ms, total: 590 ms\n",
            "Wall time: 31.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyq3yLYLfEL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "475e2529-38d2-4452-fefa-41e6f2665918"
      },
      "source": [
        "candidates_ling"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('зрения', 'создании'): ('S', 'S'),\n",
              " ('изучение', 'опыта'): ('S', 'S'),\n",
              " ('наводящие', 'избегать'): ('V', 'V'),\n",
              " ('наводящие', 'следует'): ('V', 'V'),\n",
              " ('создании', 'алфавита.'): ('S', 'S'),\n",
              " ('термин', 'морфология'): ('S', 'S')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LC0cSKfdCWY",
        "colab_type": "text"
      },
      "source": [
        "Результаты не очень хорошие – выделенные пары либо вообще не являются словосочетаниями (\"населения данные\", \"зрения создании\"), либо являются правильными словосочетаниями (\"недоверие власти\", \"термин морфология\"), при этом ни одно из спорных словосочетаний (\"умолчал данные\", \"избегать вопросы\") не выделены. Во многом эти ошибки вызваны плохой работой синтаксического парсера. <br>\n",
        "Кроме того, MaltParser довольно медленный."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_f_826CfR5r",
        "colab_type": "text"
      },
      "source": [
        "## Подход 2: StanfordNLP\n",
        "Попробуем другой парсер зависимостей. <br>\n",
        "Источник: https://github.com/stanfordnlp/stanfordnlp. <br>\n",
        "Зависимостный парсер в StanfordNLP работает лучше и быстрее (см. код ниже) MaltParser, кроме того, в него уже встроен морфологический тэггер с тэгом PROPN, поэтому не нужно искать какой-то ещё тэггер или дополнительно убирать именованные сущности."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saMIUP_hx2A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stanfordnlp\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwP6VUkH2lV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('ru')\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzVuVRMrVSKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = stanfordnlp.Pipeline(lang='ru', models_dir='/root/stanfordnlp_resources')\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIpgV_bohqU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(bad_text_pol)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNl3hxPYV4km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fc6b9a08-a777-467a-bea1-170c6ed35dff"
      },
      "source": [
        "# посмотрим, как работает этот парсер\n",
        "doc.sentences[0].dependencies[0:3]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<Word index=4;text=время;lemma=время;upos=NOUN;xpos=_;feats=Animacy=Inan|Case=Acc|Gender=Neut|Number=Sing;governor=7;dependency_relation=parataxis>,\n",
              "  'case',\n",
              "  <Word index=1;text=В;lemma=в;upos=ADP;xpos=_;feats=_;governor=4;dependency_relation=case>),\n",
              " (<Word index=4;text=время;lemma=время;upos=NOUN;xpos=_;feats=Animacy=Inan|Case=Acc|Gender=Neut|Number=Sing;governor=7;dependency_relation=parataxis>,\n",
              "  'det',\n",
              "  <Word index=2;text=то;lemma=тот;upos=DET;xpos=_;feats=Case=Acc|Gender=Neut|Number=Sing;governor=4;dependency_relation=det>),\n",
              " (<Word index=2;text=то;lemma=тот;upos=DET;xpos=_;feats=Case=Acc|Gender=Neut|Number=Sing;governor=4;dependency_relation=det>,\n",
              "  'discourse',\n",
              "  <Word index=3;text=же;lemma=же;upos=PART;xpos=_;feats=_;governor=2;dependency_relation=discourse>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMNVobbhrTtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search(text, collocations):\n",
        "    \"\"\"text – строка, collocations – датафрейм pandas.\n",
        "    Возвращается словарь candidates,\n",
        "    где ключи – пары слов, а значения – пары тэгов\"\"\"\n",
        "    candidates = {}\n",
        "    first = list(collocations.first_word)\n",
        "    second = list(collocations.second_word)\n",
        "    clean_text = re.sub(\"\\([a-zA-z\\d .,:]*\\)|[a-zA-z\\d]*\", \"\", text)\n",
        "    # чистим текст от библиографических ссылок\n",
        "    # и токенов, состоящих только из латиницы и цифр\n",
        "    parsed_text = nlp(clean_text)\n",
        "    for sent in parsed_text.sentences:\n",
        "        for dep in sent.dependencies:\n",
        "            if dep[0].upos in ['NOUN', 'VERB'] \\\n",
        "                    and dep[2].upos in ['NOUN', 'VERB']:\n",
        "                word_1 = dep[0].text.lower()\n",
        "                word_2 = dep[2].text.lower()\n",
        "                tag_1 = dep[0].upos\n",
        "                tag_2 = dep[2].upos\n",
        "                if word_1 in first or word_2 in second:\n",
        "                    if word_1 in first and word_2 in second:\n",
        "                        indices_1 = [i for i, x in enumerate(first)\n",
        "                                     if x == word_1]\n",
        "                        indices_2 = [i for i, x in enumerate(second)\n",
        "                                     if x == word_2]\n",
        "                        if not set(indices_1).isdisjoint(indices_2):\n",
        "                            pass\n",
        "                        else:\n",
        "                            candidates[(word_1, word_2)] = (tag_1, tag_2)\n",
        "                    else:\n",
        "                        candidates[(word_1, word_2)] = (tag_1, tag_2)\n",
        "    return candidates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aRtbx6NZ1g7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87b045fb-0114-4b6b-be3b-2fd50c7593c6"
      },
      "source": [
        "%%time\n",
        "candidates_pol = search(bad_text_pol, pd.read_csv('suggestions_pol.csv'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 132 ms, sys: 6 ms, total: 138 ms\n",
            "Wall time: 141 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFMvNFfDfkgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7d81c94b-e051-4637-92f6-3bcbfe29b0b7"
      },
      "source": [
        "candidates_pol"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('выросли', 'расходы'): ('VERB', 'NOUN'),\n",
              " ('умолчал', 'время'): ('VERB', 'NOUN'),\n",
              " ('умолчал', 'данные'): ('VERB', 'NOUN'),\n",
              " ('умолчал', 'населения'): ('VERB', 'NOUN')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAKP6hxBaHpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "234747e8-99af-48de-cf97-6a6673988ed2"
      },
      "source": [
        "%%time\n",
        "candidates_hist = search(bad_text_hist, pd.read_csv('suggestions_hist.csv'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 128 ms, sys: 3 ms, total: 131 ms\n",
            "Wall time: 133 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPr4vO9UfqAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "749552d3-d053-42af-aa07-966557723b57"
      },
      "source": [
        "candidates_hist"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('исследование', 'движений'): ('NOUN', 'NOUN'),\n",
              " ('недоверие', 'власти'): ('NOUN', 'NOUN'),\n",
              " ('ограничивается', 'маккензи'): ('VERB', 'NOUN'),\n",
              " ('ограничивается', 'перечисляет'): ('VERB', 'VERB'),\n",
              " ('ограничивается', 'рамками'): ('VERB', 'NOUN'),\n",
              " ('перечисляет', 'причины'): ('VERB', 'NOUN'),\n",
              " ('посвящает', 'книгу'): ('VERB', 'NOUN'),\n",
              " ('посвящает', 'частью'): ('VERB', 'NOUN'),\n",
              " ('рамками', 'исследования'): ('NOUN', 'NOUN'),\n",
              " ('рассматривать', 'год'): ('VERB', 'NOUN'),\n",
              " ('частью', 'исследование'): ('NOUN', 'NOUN')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x54PidiaTS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b48837bf-7e9d-4fed-e23f-8b230faef64c"
      },
      "source": [
        "%%time\n",
        "candidates_ling = search(bad_text_ling, pd.read_csv('suggestions_ling.csv'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 168 ms, sys: 2.01 ms, total: 170 ms\n",
            "Wall time: 177 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZr5EHPKfxkz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "03ccea1f-5f0b-4705-9a68-57e36cd5163b"
      },
      "source": [
        "candidates_ling"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('вопросы', 'наводящие'): ('NOUN', 'VERB'),\n",
              " ('грамматики', 'языка'): ('NOUN', 'NOUN'),\n",
              " ('избегать', 'вопросы'): ('VERB', 'NOUN'),\n",
              " ('изучение', 'опыта'): ('NOUN', 'NOUN'),\n",
              " ('опыта', 'ученых'): ('NOUN', 'NOUN'),\n",
              " ('предлагая', 'гипотезу'): ('VERB', 'NOUN'),\n",
              " ('следует', 'избегать'): ('VERB', 'VERB'),\n",
              " ('создании', 'алфавита'): ('NOUN', 'NOUN'),\n",
              " ('соотносится', 'термин'): ('VERB', 'NOUN'),\n",
              " ('соотносится', 'части'): ('VERB', 'NOUN'),\n",
              " ('ссылаться', 'результаты'): ('VERB', 'NOUN'),\n",
              " ('термин', 'истории'): ('NOUN', 'NOUN'),\n",
              " ('термин', 'морфология'): ('NOUN', 'NOUN'),\n",
              " ('точек', 'создании'): ('NOUN', 'NOUN'),\n",
              " ('употреблялся', 'значении'): ('VERB', 'NOUN'),\n",
              " ('употреблялся', 'термин'): ('VERB', 'NOUN'),\n",
              " ('части', 'грамматики'): ('NOUN', 'NOUN')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3O9CrKcruQ",
        "colab_type": "text"
      },
      "source": [
        "Уже лучше! Все предложенные пары слов являются словосочетаниями (хотя функция *search()* всё ещё иногда ошибочно отлавливает правильные коллокации)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKoBr6CIa8vU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
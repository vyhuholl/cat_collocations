{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cat_collocations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StMZX2azxXHl",
        "colab_type": "text"
      },
      "source": [
        "Задача: научиться находить в тексте лексически/стилистически неправильные сочетания (коллокации) и предлагать более правильные замены. <br>\n",
        "Алгоритм является улучшенной версией этого алгоритма https://github.com/annadmitrieva/NLP-stuff/blob/master/collocation_generation-Copy4.ipynb "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC5oMw4Dw6NZ",
        "colab_type": "text"
      },
      "source": [
        "#Этапы работы\n",
        "\n",
        "*   Автоматическое обнаружение \"неправильных\" коллокаций в тексте.\n",
        "*   Поиск замен."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznC-an5e5n7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P3jg0IeSc6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install natasha\n",
        "!pip install treetaggerwrapper\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DEhQpYgQcXj",
        "colab_type": "text"
      },
      "source": [
        "Скачиваем word2vec модель из rusvectores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwd70RgHSlbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores4/ruwikiruscorpora/ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgD2XXsOQhSy",
        "colab_type": "text"
      },
      "source": [
        "Для определения домена текста используем предобученную на шестиграммах из каждого домена (по 100000 на домен) SVM. F1-score модели – 0.94. <br>\n",
        "Код модели: https://github.com/annadmitrieva/NLP-stuff/blob/master/domain_model.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av2Q34pcTLa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/lyfk0ur0pqvcgqv/domain_model.pkl?dl=0 -O domain_model.pkl\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxSRwGTUWp6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/y35dm5chuv54psg/suggestions.zip?dl=0 -O suggestions.zip\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aisa0dw-W1Yt",
        "colab_type": "code",
        "outputId": "f82d4c5d-b559-4f17-da40-92ec82944f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "!unzip suggestions.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  suggestions.zip\n",
            "  inflating: suggestions_law.csv     \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._suggestions_law.csv  \n",
            "  inflating: suggestions_ling.csv    \n",
            "  inflating: __MACOSX/._suggestions_ling.csv  \n",
            "  inflating: suggestions_pol.csv     \n",
            "  inflating: __MACOSX/._suggestions_pol.csv  \n",
            "  inflating: suggestions_hist.csv    \n",
            "  inflating: __MACOSX/._suggestions_hist.csv  \n",
            "  inflating: suggestions_psy.csv     \n",
            "  inflating: __MACOSX/._suggestions_psy.csv  \n",
            "  inflating: suggestions_soc.csv     \n",
            "  inflating: __MACOSX/._suggestions_soc.csv  \n",
            "  inflating: suggestions_ec.csv      \n",
            "  inflating: __MACOSX/._suggestions_ec.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KeVWEZsTXLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from treetaggerwrapper import TreeTagger\n",
        "from natasha import NamesExtractor, AddressExtractor, DatesExtractor\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz', binary=False)\n",
        "domain_model=joblib.load('domain_model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig3R2wvUpuMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b721502b-7be1-4fa6-dbd3-71b635162dfa"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phW6KHcuXcQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def domain(string):\n",
        "    domain=domain_model.predict(list(string))[0]\n",
        "    df=pd.read_csv(f'suggestions_{domain}.csv')\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET70OxkDcjI5",
        "colab_type": "text"
      },
      "source": [
        "#Поиск неправильных сочетаний"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4hhc4q_VnoM",
        "colab_type": "text"
      },
      "source": [
        "Для начала в тексте находятся все пары \"вершина-зависимое\", которые состоят из существительных и глаголов и которых нет в эталонных списках правильных словосочетаний (для каждого домена текста – свой список).<br>\n",
        "Для морфологической разметки использовалась питоновская обёртка для TreeTagger.<br>\n",
        "Для синтаксического парсинга использовался MaltParser из NLTK с предобученной моделью для русского языка. <br>\n",
        "Источник модели: http://corpus.leeds.ac.uk/mocky/ <br>\n",
        "Эталонные списки коллокаций: https://drive.google.com/drive/folders/1k_N-DZ-nLL5ro66-LxIaE4-dRwirdwZh <br>\n",
        "Код, которых использовался при сборе эталонных списков: https://github.com/MariaFjodorowa/catandthekittens/tree/develop/collocations/collocation_frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxqmEIAea2CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://corpus.leeds.ac.uk/tools/russian.mco\n",
        "!wget http://maltparser.org/dist/maltparser-1.8.1.zip\n",
        "!unzip maltparser-1.8.1.zip\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsBBabRMhKnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export MALT_PARSER=$HOME/maltparser-1.8.1/\n",
        "!export MALT_MODEL=$HOME/russian.mco"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGk1vb92ipvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "syntax_parser = nltk.parse.malt.MaltParser('maltparser-1.8.1', 'russian.mco')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6x_yisGUKzt",
        "colab_type": "text"
      },
      "source": [
        "Несколько плохих текстов, на которых будем проверять, насколько хорошо находятся плохие коллокации. Источник – примеры плохой лексической сочетаемости из домашнего задания по академическому письму 1 курса ОП \"Фундаментальная и прикладная лингвистика\" НИУ ВШЭ 2017 года."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wTiBajJlZ3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad_text_pol = \"\"\"В то же время, Путин умолчал от египетского населения данные о том, \n",
        "насколько милитаризируется сама Россия и насколько выросли ее военные расходы. \n",
        "Обама улетел из Вашингтона на вертолете, а Байден - на поезде.\"\"\"\n",
        "bad_text_hist = \"\"\"Маккензи ограничивается рамками исследования 1928 – 1943, исследуя преимущественно идеологию Коминтерна сталинской эпохи, \n",
        "перечисляет причины на то, чтобы рассматривать 1928 год как переломный. \n",
        "Александр Шубин посвящает свою книгу большей частью на исследование общественно-политических движений в послесталинский СССР, \n",
        "1953 – 85 годов. В строках читаются напряжение и недоверие власти.\"\"\"\n",
        "bad_text_ling = \"\"\"Следует избегать наводящие вопросы. Тем не менее существовало несколько точек зрения о создании алфавита. \n",
        "Предлагая ту или иную гипотезу, нельзя основываться и ссылаться только на результаты своего собственного исследования. \n",
        "Необходимо внимательное изучение опыта других ученых. \n",
        "Термин морфология обычно соотносится к части грамматики языка. Однако этот термин в истории науки употреблялся и в совсем ином значении.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm1TcbjMrnJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "69c423b3-09fa-4dc7-8e00-db85f68445a4"
      },
      "source": [
        "#проверим, правильно ли работает парсер\n",
        "sents = nltk.sent_tokenize(bad_text_pol)\n",
        "nodes = syntax_parser.parse_one(sents[0].split()).nodes\n",
        "for node in nodes.keys():\n",
        "    head = nodes[node]['head']\n",
        "    if head is not None and nodes[head]['word'] is not None:\n",
        "        print(f\"Head: {nodes[head]['word']} {nodes[head]['ctag']}, dependent: {nodes[node]['word']} {nodes[node]['ctag']}.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Head: то NN, dependent: В NN.\n",
            "Head: то NN, dependent: же NN.\n",
            "Head: Путин NN, dependent: время, NN.\n",
            "Head: умолчал NN, dependent: Путин NN.\n",
            "Head: от NN, dependent: умолчал NN.\n",
            "Head: то NN, dependent: от NN.\n",
            "Head: от NN, dependent: египетского NN.\n",
            "Head: египетского NN, dependent: населения NN.\n",
            "Head: о NN, dependent: данные NN.\n",
            "Head: том, NN, dependent: о NN.\n",
            "Head: насколько NN, dependent: том, NN.\n",
            "Head: милитаризируется NN, dependent: насколько NN.\n",
            "Head: сама NN, dependent: милитаризируется NN.\n",
            "Head: Россия NN, dependent: сама NN.\n",
            "Head: и NN, dependent: Россия NN.\n",
            "Head: насколько NN, dependent: и NN.\n",
            "Head: выросли NN, dependent: насколько NN.\n",
            "Head: ее NN, dependent: выросли NN.\n",
            "Head: военные NN, dependent: ее NN.\n",
            "Head: то NN, dependent: военные NN.\n",
            "Head: военные NN, dependent: расходы. NN.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/parse/dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
            "  \"The graph doesn't contain a node \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OCtnuWCoFQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search(text, collocations):\n",
        "    clean_text = re.sub(\"\\([a-zA-z\\d .,:]*\\)|[a-zA-z\\d]*\", \"\", text)\n",
        "    #чистим текст от библиографических ссылок и токенов, состоящих только из латиницы и цифр\n",
        "    sents = nltk.sent_tokenize(clean_text)\n",
        "    #разбиваем на предложения\n",
        "    for parse in syntax_parser.parse_sents(sents):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkYt7PDLirB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}